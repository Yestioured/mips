===========================================================================
------------------------------- INSTRUCTIONS ------------------------------
===========================================================================

___ CODE STRUCTURE ________________________________________________________

Launch 'INSTALL' to add the path of all folders:

- 'Algs' contains all the algorithms,
- 'Library' contains all the policies, solvers, general basis functions, 
   and functions for sampling and evaluation,
- 'MDPs' contains the models and the simulators of the MPDs,
- 'MO_Library' contains functions used in the Multi-Objective framework,
- 'Utilities' contains utility functions.


___ HOW TO ADD A NEW MDP __________________________________________________

Each MDP requires some mandatory functions:

- NAME_mdpvariables : defines the details of the problem (number of state 
                      variables, actions, rewards, discount factor, ...),
- NAME_simulator    : defines the reward and transition functions,
- NAME_settings     : defines the learning setup, i.e., the policy to use 
                      and the number of episodes and steps used for 
                      evaluation / learning.

Please notice that you have to manually change the number of episodes and 
steps in NAME_settings.m according to your needs.

Additionally, there are some functions used to better organize the code:

- NAME_environment  : defines additional details of the problem environment,
- NAME_basis        : defines the features used to represent a state,
- NAME_moref        : returns all the details related to the Multi-Objective 
                      setup, i.e., the reference frontier used for 
                      comparison, the utopia and antiutopia points. It also
                      returns a set of weights if the reference front is 
                      obtained with a weighted scalarization of the 
                      objectives. Both the frontier and the weights are 
                      saved in NAME_ref.dat and NAME_w.dat, respectively.

Finally, the function 'settings_episodic' is used as a wrapper to set up  
the learning for episodic algorithms. Modify this function only to specify 
the distribution used to collect samples (e.g., a Gaussian with diagonal 
covariance or a Gaussian Mixture Model).


___ RELE INTERFACE ________________________________________________________

For collecting samples and computing gradients and hessians, you can also 
use ReLe, a powerful toolbox in C. 
You can find it here: https://github.com/AIRLab-POLIMI/ReLe

First, you need to mex the files in '/ReLe/rele_matlab/src/MEX_functions'.
Then add such folder to Matlab search path.
Finally just call 'collect_samples_rele' instead of 'collect_samples'.




===========================================================================
---------------------------------- NOTES ----------------------------------
===========================================================================

With episode-based algorithms (e.g. REPS, PGPE, NES) it is recommended to 
make the lower-level policy deterministic (use 'policy.makeDeterministic').
The reason is that such algorithms cannot deal with high stochasticity, as 
each parameter vector is evaluated only on one episode and therefore its
estimate would be inaccurate, unless we use a lot of samples to learn. 
Also, for Gaussian with full covariance (without Cholesky decomposition) 
the covariance matrix generated by the upper-level policy could be 
not positive-semidefinite (SPD).

Anyway, they can manage low stochasticity and if you want your low-level 
policy to be stochastic you can of course do it.
___________________________________________________________________________

In episodic policy search (e.g. REPS, NES) you can use samples from the 
last N_MAX policies to stabilize the policy update. This stabilization is 
important to keep the shape of the explorative variance of the policy in 
high dimensional action spaces. However, it also slows down the convergence 
speed (the KL divergence will be 0 only if the last N_MAX samples are 
optimal, i.e., if the variance of the final policy is very low).
___________________________________________________________________________

In PFA and RA, while checking for the minimal-norm Pareto-ascent direction, 
the gradients are always normalized (i.e., we normalize the gradients even 
if 'norm(grad_i) < 1'). If the gradients are not normalized, then the 
minimial-norm Pareto-ascent direction is expected to be mostly influenced 
by the elements which have smaller norms. Since gradients with small norm 
are usually associated with objectives that have already achieved a fair 
degree of convergence, the utility of considering these directions for a 
well-balanced correction is questionable. 
This observation points out the necessity of normalizing ALL the gradients 
when looking for the minimal-norm Pareto-ascent direction (Desideri, 
Multiple-gradient descent algorithm for multiobjective optimization, 2012).
___________________________________________________________________________

PFA (usually) needs a randomization of the policy after the optimization of 
one objective. Such randomization is needed to guarantee enough exploration 
to optimize the remaining objectives and depends on the policy used. 

For instance: 
- for a Gibbs policy we can reduce the temperature or scale theta with a 
  constant factor (e.g., we can halve theta),
- for a Gaussian policy we can reset the covariance matrix at its initial 
  value or we can, again, scale it.

Using the entropy as a scaling factor seems the most reasonable choice, but 
for Gaussians the (differential) entropy can be negative, while for Gibbs 
policies it could be too low.
___________________________________________________________________________

When evaluating a policy with 'evaluate_policies', you can choose to make 
it deterministic or not. In MORL, this affects the resulting Pareto 
frontier, as any convex combination of the points one the deterministic 
frontier belongs to the stochastic frontier.
Of course, considering only deterministic policies is much less time 
consuming, as only 1 episode is required to evaluate it (except when the
environment is stochastic).
